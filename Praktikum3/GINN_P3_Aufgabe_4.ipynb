{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Praktikum 3: Softmax, Regularisierung, Momentum und Batchsize\n",
    "## 4. Eigener Datensatz\n",
    "Suchen Sie sich eine verfügbare PyTorch MLP-Implementierung für einen Datensatz heraus, der Sie besonders interessiert. Reproduzieren Sie die Ergebnisse lokal, auf colab oder RosettaHub. Ist die Implementierung vergleichbar mit der hier gegebenen IMDB Implementierung? Wo unterscheidet sich diese und ggf. warum? Wie verändern Sich die Ergebnisse, wenn Sie bestimmte Hyperparameter verändern?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelle: https://github.com/christianversloot/machine-learning-articles/blob/main/creating-a-multilayer-perceptron-with-pytorch-and-lightning.md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from torcheval.metrics import MulticlassAccuracy, MulticlassPrecision, MulticlassRecall\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unterschied zu IMDB Implementierung:\n",
    "- hier feste Neuronenanzahl je Layer, andere werden erst bei Initialisierung festgelegt\n",
    "- vorher flatten() ausgeführt, bei uns wird x_train vorher in torch umgewandelt\n",
    "- ReLU statt sigmoid verwendet\n",
    "- keine Aktivierungsfunktion im letzten Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "  '''\n",
    "    Multilayer Perceptron.\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "      nn.Flatten(),\n",
    "      nn.Linear(32 * 32 * 3, 64),\n",
    "      nn.ReLU(), # original\n",
    "      # nn.Sigmoid(), \n",
    "      nn.Linear(64, 32),\n",
    "      nn.ReLU(), # original\n",
    "      # nn.Sigmoid(), \n",
    "      nn.Linear(32, 10),\n",
    "      # nn.Softmax() #selbst hinzugefügt\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''Forward pass'''\n",
    "    return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unterschied zu IMDB Implementierung:\n",
    "- Funktion zur Erstellung der batches selbst definiert\n",
    "- Es werden keine Testdaten verwendet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare CIFAR-10 dataset\n",
    "dataset = CIFAR10(os.getcwd(), download=True, transform=transforms.ToTensor())\n",
    "trainloader = torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unterschied zu IMDB Implementierung:\n",
    "- Adam statt SGD optimizer\n",
    "- beide verwenden CrossEnropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MLP\n",
    "mlp = MLP()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)# vorher 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary über model\n",
    "from torchinfo import summary\n",
    "summary(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unterschied zu IMDB Implementierung:\n",
    "- kaum Unterschied zur IMDB Implementierung\n",
    "- Epochenanzahl festgelegt und nicht variabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_list = [] \n",
    "# # Run the training loop\n",
    "# for epoch in range(0, 5): # 5 epochs at maximum\n",
    "\n",
    "#     # Print epoch\n",
    "#     print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "#     # Set current loss value\n",
    "#     current_loss = 0.0\n",
    "\n",
    "#     # Iterate over the DataLoader for training data\n",
    "#     for i, data in enumerate(trainloader, 0):\n",
    "        \n",
    "#         # Get inputs\n",
    "#         inputs, targets = data\n",
    "        \n",
    "#         # Zero the gradients\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         # Perform forward pass\n",
    "#         outputs = mlp(inputs)\n",
    "        \n",
    "#         # Compute loss\n",
    "#         loss = loss_function(outputs, targets)\n",
    "        \n",
    "#         # Perform backward pass\n",
    "#         loss.backward()\n",
    "        \n",
    "#         # Perform optimization\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         loss_list.append(loss.item())\n",
    "\n",
    "#         # Print statistics\n",
    "#         current_loss += loss.item()\n",
    "#         if i % 500 == 499:\n",
    "#             print('Loss after mini-batch %5d: %.3f' %\n",
    "#                 (i + 1, current_loss / 500))\n",
    "#             current_loss = 0.0\n",
    "\n",
    "#     # Process is complete.\n",
    "# print('Training process has finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training zeichnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list = [] \n",
    "loss_dict = dict()\n",
    "correct = []\n",
    "\n",
    "accuracy = MulticlassAccuracy(average=None, num_classes=10)\n",
    "precision = MulticlassPrecision(average=None, num_classes=10)\n",
    "recall = MulticlassRecall(average=None, num_classes=10)\n",
    "\n",
    "accuracy_list = []\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "\n",
    "# Run the training loop\n",
    "for epoch in range(0, 5): # 5 epochs at maximum\n",
    "\n",
    "    accuracy.reset()\n",
    "    precision.reset()\n",
    "    recall.reset()\n",
    "\n",
    "    # Print epoch\n",
    "    print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "    # Set current loss value\n",
    "    current_loss = 0.0\n",
    "\n",
    "    # Iterate over the DataLoader for training data\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        \n",
    "        # Get inputs\n",
    "        inputs, targets = data\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Perform forward pass\n",
    "        outputs = mlp(inputs)\n",
    "        \n",
    "        # Compute loss with onehot encoded values\n",
    "        loss = loss_function(outputs,targets)\n",
    "        \n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        # Print statistics\n",
    "        current_loss += loss.item()\n",
    "        if i % 500 == 499:\n",
    "            print('Loss epoch %d after mini-batch %5d: %.3f' %\n",
    "                (epoch, i + 1, current_loss / 500))\n",
    "            if epoch == 0:\n",
    "                loss_dict[i] = [current_loss/500]\n",
    "            else:\n",
    "                loss_dict[i].append(current_loss/500)\n",
    "            current_loss = 0.0\n",
    "    \n",
    "        accuracy.update(torch.argmax(outputs, dim=0), targets)\n",
    "        acc = accuracy.compute()\n",
    "        precision.update(torch.argmax(outputs, dim=0), targets)\n",
    "        prec = precision.compute()\n",
    "        recall.update(torch.argmax(outputs, dim=0), targets)\n",
    "        rec = recall.compute()\n",
    "        \n",
    "        # accuracy.compute() returns accuracy value from all seen data\n",
    "    print(f'Metriken Epoche {epoch+1}: Accuracy - {acc}, Precision - {prec}, Recall - {rec}')\n",
    "    \n",
    "    accuracy_list.append(acc)\n",
    "    precision_list.append(prec)\n",
    "    recall_list.append(rec)\n",
    "    \n",
    "    # Process is complete.\n",
    "print('Training process has finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy ist etwas besser als Zufall (0.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #plot changing loss for each epoch\n",
    "# labels = range(1, len(loss_dict[499])+1)\n",
    "# plt.plot(loss_dict.keys(), loss_dict.values(),label = labels)\n",
    "# plt.legend(title = 'epoch')\n",
    "# plt.title('Durchschnittlicher Loss aus 500 Trainingsschritten \\nje Epoche')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Damit Grafik mit jeder Epochenanzahl funktioniert\n",
    "epoch_count = len(loss_list) // 5000  # Ganzzahldivision\n",
    "loss_list_chunked = np.array_split(loss_list, epoch_count)\n",
    "\n",
    "# Transponieren, da über Spalten nicht Zeilen gemittelt werden soll\n",
    "t_average = pd.DataFrame(loss_list_chunked).T.rolling(window=500).mean().T\n",
    "\n",
    "for i in range(epoch_count):\n",
    "    plt.plot(range(1,5001), t_average.iloc[i,:],label = i+1, alpha=0.3)\n",
    "plt.legend(title = 'epoch')\n",
    "plt.title('Gleitender Durchschnitt des Loss je Epoche')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    plt.scatter(range(10),accuracy_list[i], label = i)\n",
    "plt.legend(title = 'Epoche')\n",
    "plt.xticks(range(10))\n",
    "plt.title('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    plt.scatter(range(10),recall_list[i], label = i)\n",
    "plt.legend(title = 'Epoche')\n",
    "plt.xticks(range(10))\n",
    "plt.title('Recall')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    plt.scatter(range(10),precision_list[i], label = i)\n",
    "plt.legend(title = 'Epoche')\n",
    "plt.xticks(range(10))\n",
    "plt.title('Precision')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GINN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
