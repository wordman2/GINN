{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Praktikum 3: Softmax, Regularisierung, Momentum und Batchsize\n",
    "## 4. Eigener Datensatz\n",
    "Suchen Sie sich eine verf체gbare PyTorch MLP-Implementierung f체r einen Datensatz heraus, der Sie besonders interessiert. Reproduzieren Sie die Ergebnisse lokal, auf colab oder RosettaHub. Ist die Implementierung vergleichbar mit der hier gegebenen IMDB Implementierung? Wo unterscheidet sich diese und ggf. warum? Wie ver채ndern Sich die Ergebnisse, wenn Sie bestimmte Hyperparameter ver채ndern?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelle: https://github.com/christianversloot/machine-learning-articles/blob/main/creating-a-multilayer-perceptron-with-pytorch-and-lightning.md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 127] Die angegebene Prozedur wurde nicht gefunden. Error loading \"c:\\Users\\anika\\anaconda3\\envs\\GINN\\lib\\site-packages\\torch\\lib\\c10_cuda.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# from torch import nn\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# # from torchvision.datasets import CIFAR10\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# from torch.utils.data import DataLoader\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# # from torchvision import transforms\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anika\\anaconda3\\envs\\GINN\\lib\\site-packages\\torch\\__init__.py:130\u001b[0m\n\u001b[0;32m    128\u001b[0m     err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(last_error)\n\u001b[0;32m    129\u001b[0m     err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 130\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    132\u001b[0m     is_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 127] Die angegebene Prozedur wurde nicht gefunden. Error loading \"c:\\Users\\anika\\anaconda3\\envs\\GINN\\lib\\site-packages\\torch\\lib\\c10_cuda.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "# from torch import nn\n",
    "# # from torchvision.datasets import CIFAR10\n",
    "# from torch.utils.data import DataLoader\n",
    "# # from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "  '''\n",
    "    Multilayer Perceptron.\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "      nn.Flatten(),\n",
    "      nn.Linear(32 * 32 * 3, 64),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(64, 32),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(32, 10)\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''Forward pass'''\n",
    "    return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare CIFAR-10 dataset\n",
    "dataset = CIFAR10(os.getcwd(), download=True, transform=transforms.ToTensor())\n",
    "trainloader = torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MLP\n",
    "mlp = MLP()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Run the training loop\n",
    "for epoch in range(0, 5): # 5 epochs at maximum\n",
    "\n",
    "    # Print epoch\n",
    "    print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "    # Set current loss value\n",
    "    current_loss = 0.0\n",
    "\n",
    "    # Iterate over the DataLoader for training data\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        \n",
    "        # Get inputs\n",
    "        inputs, targets = data\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Perform forward pass\n",
    "        outputs = mlp(inputs)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_function(outputs, targets)\n",
    "        \n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print statistics\n",
    "        current_loss += loss.item()\n",
    "        if i % 500 == 499:\n",
    "            print('Loss after mini-batch %5d: %.3f' %\n",
    "                (i + 1, current_loss / 500))\n",
    "            current_loss = 0.0\n",
    "\n",
    "    # Process is complete.\n",
    "print('Training process has finished.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GINN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
