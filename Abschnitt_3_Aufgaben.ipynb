{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wordman2/GINN/blob/main/Abschnitt_3_Aufgaben.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10x48mgFIJHt"
      },
      "source": [
        "# Imports\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uF7V3nLFIKyi"
      },
      "source": [
        "# GINN Teil 3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_NwnsZddYAc"
      },
      "source": [
        "\n",
        "\n",
        "*   $\\frac{{\\partial J_S \\left( \\theta  \\right)}}{{\\partial J\\left( \\theta  \\right)}} = \\frac{1}{{\\left| S \\right|}}\\sum\\limits_{\\left( {x,y} \\right) \\in S} {\\frac{{\\partial J\\left( {x,y\\theta } \\right)}}{{\\partial \\left( \\theta  \\right)}}} $ Gradient der Zielfunktion als Durchschnitt der Einzelgradienten\n",
        "*   $B \\subseteq S $: Batch\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iXtG39lKe1a"
      },
      "source": [
        "## Aufgaben\n",
        "\n",
        "\n",
        "\n",
        "*   Begründen Sie, ob wahr oder falsch:\n",
        "  *   Für ein Batch Update müssen alle Daten einmal durchlaufen worden sein\n",
        "  *   Batch Updates konvergieren grundsätzlich schneller als Stochastic Gradient Descent\n",
        "  *   Bei schlechtem Lernverhalten von Batch Updates kann es helfen, die Daten vorher zu mischen.\n",
        "    *   Bei schlechtem Lernverhalten von Stochastic Gradient Descent  kann es helfen, die Daten vorher zu mischen.\n",
        "  *   Minibatch Updates mit einer Größe von 1 konvergieren schneller als Stochastic Gradient Descent\n",
        "* Formulieren Sie die Varianten Batch Updates, Minibatch Updates und Stochastic Gradient Descent selbst als Pseudocode\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   True\n",
        "*   True\n",
        "*   False, da das Update erst nach allen gesehenen Daten durchgeführt wird, bringt eine Mischung nichts\n",
        "*   True, wenn das Modell zuerst die Daten einer Klasse (homogene Daten) sieht, kann der Gradientenabstieg von ihnen beeinflusst werden. Das Mischen kann das Lernverhalten möglicherweise beschleunigen\n",
        "*   False Minibatch mit einer Größe von 1 == SGD\n",
        "\n"
      ],
      "metadata": {
        "id": "IFevKr168NmJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPtvR1rO8Io7"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# Batch\n",
        "\n",
        "Für jede Episode\n",
        "  für jedes Datensample\n",
        "    berechne den Gradient Descent\n",
        "  update Theta mithilfe aller Einzelgradienten\n",
        "\n",
        "\n",
        "# Mini Batch\n",
        "\n",
        "für jede Episode\n",
        "  für jeden batch\n",
        "    für jedes Datensample im Batch\n",
        "      berechne Gradient Descent\n",
        "    update Theta mithilfe der im Batch berechneten Gradienten\n",
        "\n",
        "# Stoch\n",
        "\n",
        "für jede Episode\n",
        "  für jedes Datensample\n",
        "    berechne Gradient Descent\n",
        "    update Theta\n",
        "\"\"\"\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}