{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wordman2/GINN/blob/main/Colab_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GINN Teil 8"
      ],
      "metadata": {
        "id": "evnJ-Y7az9VO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "DTaUNZcp0B0a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiple Choice\n",
        "\n",
        "Welche der Aussagen ist wahr?\n",
        "\n",
        "\n",
        "\n",
        "1.   Dropout führt zu Sparsity in den trainierten Modellgewichten (True)\n",
        "2.   Beim Testen auf Validierungsdaten wird Droput mit umgekehrter Wahrscheinlichkeit p angewendet (False, no dropout applied during validation)\n",
        "3. Je kleiner die Dropout-Wahrscheinlichkeit p, desto stärker der Regularisierungseffekt auf die Gewichte in den Layern (False, da bernoulli 1-p gezogen wird. Effekt ist umgekehrt)\n",
        "4. Aussagen 1-3 sind nicht wahr\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H2_5lYOg9GSF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PyTorch Dropout zu Fuß"
      ],
      "metadata": {
        "id": "-l22JNOQHxdF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementieren Sie den PyTorch Dropout zu Fuß und stellen Sie sicher, dass Ihre Ergebnisse mit denen von PyTorch identisch sind (hört sich wahrscheinlich erstmal einfacher an als es ist :)\n",
        "\n",
        "\n",
        "\n",
        "Hinweis: PyTorch implementiert den sogenannten \"Inverted Dropout\" wie hier besschrieben: https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html, bei dem auf Grund des korrigierten Erwartungswertes **keine** Anpassungen zur Inferenzzeit mehr nötig sind."
      ],
      "metadata": {
        "id": "-p7ParwUHzzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ihr Code hier\n",
        "from torch.nn import Dropout\n",
        "import torch\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "m = Dropout(p=0.2)\n",
        "input = torch.randn(20, 16)\n",
        "output = m(input)\n",
        "\n",
        "def Dropout_by_foot(tensor, p=0.2):\n",
        "  p_matrix = torch.full_like(tensor, 1-p)\n",
        "  bernoulli_draw = torch.bernoulli(p_matrix)\n",
        "  output = (tensor*bernoulli_draw)*(1/(1-p)) # scaling\n",
        "  return output\n",
        "\n",
        "Dropout_by_foot(input) == output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rLohLyT9FKI",
        "outputId": "1fd4501f-5f9d-4ee4-fcbb-f0eb9f336044"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ True,  True,  True,  True, False,  True,  True,  True,  True,  True,\n",
              "          True, False, False,  True,  True, False],\n",
              "        [False, False, False,  True,  True, False,  True,  True, False,  True,\n",
              "          True,  True, False,  True,  True, False],\n",
              "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         False,  True, False, False,  True,  True],\n",
              "        [ True,  True, False,  True,  True,  True,  True, False,  True,  True,\n",
              "          True,  True, False, False,  True,  True],\n",
              "        [False,  True, False,  True, False,  True,  True,  True,  True, False,\n",
              "         False, False,  True, False, False,  True],\n",
              "        [ True,  True,  True,  True,  True, False, False,  True, False,  True,\n",
              "          True, False,  True,  True,  True,  True],\n",
              "        [ True,  True, False, False,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True],\n",
              "        [ True,  True, False,  True,  True,  True,  True,  True,  True,  True,\n",
              "         False,  True, False,  True, False,  True],\n",
              "        [ True, False, False, False,  True,  True,  True,  True,  True, False,\n",
              "         False,  True,  True,  True,  True, False],\n",
              "        [ True,  True,  True,  True, False,  True, False,  True,  True,  True,\n",
              "          True, False,  True,  True,  True,  True],\n",
              "        [ True, False,  True,  True,  True, False,  True,  True,  True,  True,\n",
              "          True,  True,  True, False,  True,  True],\n",
              "        [False, False, False,  True, False,  True, False,  True,  True,  True,\n",
              "          True,  True, False,  True,  True,  True],\n",
              "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True, False,  True,  True],\n",
              "        [False,  True, False,  True, False,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True],\n",
              "        [False,  True, False,  True, False,  True,  True, False,  True,  True,\n",
              "          True,  True,  True, False,  True,  True],\n",
              "        [ True,  True,  True,  True, False,  True,  True, False, False, False,\n",
              "          True, False,  True,  True, False,  True],\n",
              "        [ True,  True, False, False, False,  True,  True, False,  True,  True,\n",
              "          True, False, False,  True,  True,  True],\n",
              "        [False,  True,  True,  True, False, False, False,  True,  True,  True,\n",
              "         False, False, False,  True,  True,  True],\n",
              "        [ True,  True,  True, False, False,  True, False,  True,  True,  True,\n",
              "          True,  True, False, False, False,  True],\n",
              "        [ True,  True,  True,  True,  True, False,  True,  True,  True, False,\n",
              "          True, False,  True,  True,  True, False]])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVdN2RoHDC-m",
        "outputId": "7f502532-d3cd-4338-90a9-75099d6e7798"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.4073e+00, -1.4405e+00, -3.1322e-01, -0.0000e+00,  1.0609e+00,\n",
              "          8.6501e-01, -3.9502e-01, -2.6440e+00,  4.0284e-01, -1.5792e+00,\n",
              "          4.3748e-01,  3.8517e-01,  1.4980e-01,  1.5471e+00,  1.3960e+00,\n",
              "         -3.0910e-01],\n",
              "        [-1.6908e+00, -2.1199e+00,  7.0831e-01,  9.9189e-01,  7.4855e-01,\n",
              "         -1.9439e+00, -4.2670e-01,  2.3163e+00,  9.3774e-01, -7.3187e-01,\n",
              "         -2.1675e-01,  2.2935e-01,  0.0000e+00,  1.9829e+00,  0.0000e+00,\n",
              "         -0.0000e+00],\n",
              "        [-7.6698e-01,  3.9491e-02, -6.1585e-01,  3.1052e-01,  5.4962e-01,\n",
              "          1.4051e-01,  8.0099e-01,  5.5145e-01, -1.2789e-01,  9.9055e-01,\n",
              "         -3.6208e-01,  6.5634e-02,  6.5358e-01,  2.8778e+00, -1.8361e+00,\n",
              "         -1.9834e+00],\n",
              "        [-8.4136e-01,  1.0910e+00,  1.3192e+00,  2.2230e-01, -2.8792e-01,\n",
              "         -4.8969e-01,  6.7912e-01, -4.9395e-01, -0.0000e+00,  0.0000e+00,\n",
              "          0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
              "         -6.8941e-01],\n",
              "        [-7.1156e-01,  1.1500e+00,  1.3885e+00,  1.6123e+00, -0.0000e+00,\n",
              "          3.2090e+00, -5.9140e-01,  4.1944e-01, -2.0367e+00, -0.0000e+00,\n",
              "         -5.9979e-01, -6.2460e-01, -1.3337e+00,  0.0000e+00, -1.7584e-01,\n",
              "          0.0000e+00],\n",
              "        [-1.1669e-01,  8.5881e-01, -0.0000e+00,  1.1148e-03,  1.0524e+00,\n",
              "         -5.0004e-01,  1.2993e+00,  4.4769e-01, -0.0000e+00,  2.8781e+00,\n",
              "         -2.3521e+00, -6.2159e-02, -1.3062e+00, -1.1956e+00,  4.1915e-02,\n",
              "          8.8761e-01],\n",
              "        [ 2.0573e+00, -1.7002e+00,  4.3071e-01,  0.0000e+00, -3.2667e+00,\n",
              "         -2.1206e+00, -2.8530e-01,  3.4994e-01,  3.0866e-01,  9.6109e-02,\n",
              "          4.2251e-01,  5.6800e-01,  5.7118e-01, -1.0817e+00,  9.7663e-01,\n",
              "         -0.0000e+00],\n",
              "        [-2.7354e-01, -3.0438e+00, -0.0000e+00, -4.2483e-02,  1.2031e+00,\n",
              "          4.3646e-01, -1.1518e+00, -7.0243e-02, -7.7837e-01, -5.7965e-01,\n",
              "          2.4022e+00, -5.0318e-01,  1.5488e-01,  1.4560e+00,  0.0000e+00,\n",
              "          1.7341e+00],\n",
              "        [-1.1042e+00, -5.2364e-01, -0.0000e+00,  7.0701e-01,  7.6296e-01,\n",
              "          5.8360e-01,  0.0000e+00, -1.3289e+00, -9.6657e-02,  0.0000e+00,\n",
              "         -0.0000e+00, -1.5549e+00, -1.2762e-01, -1.2919e+00, -3.9080e-01,\n",
              "          0.0000e+00],\n",
              "        [-3.2455e-01,  1.4792e-01,  3.0495e-01,  1.4558e+00,  0.0000e+00,\n",
              "          4.8325e-01, -2.5133e-01, -1.4741e-01,  2.4025e-01, -9.6520e-01,\n",
              "         -2.3754e+00,  0.0000e+00, -8.8037e-01,  3.9340e-01,  1.9674e-01,\n",
              "          4.8170e-01],\n",
              "        [ 1.2089e+00, -0.0000e+00,  3.7701e-01, -1.3415e-01,  0.0000e+00,\n",
              "         -6.2339e-01,  9.5139e-01,  7.7288e-01,  3.9256e-01,  2.6667e-01,\n",
              "         -1.5006e-01,  4.5057e-01, -3.9254e-01, -1.3484e+00,  3.0101e-01,\n",
              "         -1.7453e+00],\n",
              "        [-0.0000e+00, -4.4794e-01, -1.9520e+00, -4.4330e-01,  1.3513e+00,\n",
              "          1.6435e-01,  0.0000e+00,  0.0000e+00, -1.3483e+00, -9.0114e-01,\n",
              "          1.8385e+00,  3.4454e-01,  0.0000e+00, -1.2430e+00, -0.0000e+00,\n",
              "         -1.4949e+00],\n",
              "        [-6.9954e-01,  6.6684e-01,  5.0861e-01,  4.9323e-01,  2.1439e-01,\n",
              "          1.0951e+00, -3.5886e-01,  1.2771e+00, -9.2994e-02, -1.3653e+00,\n",
              "          4.9003e-01,  7.4316e-01,  8.2784e-01, -1.5079e+00,  7.5930e-01,\n",
              "         -6.8395e-01],\n",
              "        [ 0.0000e+00,  0.0000e+00,  1.2042e+00,  1.0504e+00, -0.0000e+00,\n",
              "          1.2335e+00, -6.1832e-01, -1.6038e+00,  1.1940e+00,  1.6045e+00,\n",
              "         -8.3233e-01,  7.0642e-01,  3.5963e-01, -4.1718e-02, -1.3274e+00,\n",
              "         -1.4303e-01],\n",
              "        [-4.2917e-01,  1.9641e+00,  0.0000e+00,  4.7493e-01, -1.8095e-01,\n",
              "          7.9702e-01, -3.5161e-01, -1.6623e+00, -1.7751e-01, -6.6768e-01,\n",
              "         -6.5422e-01,  1.0769e+00, -1.1087e+00,  1.0485e+00,  1.4411e+00,\n",
              "         -2.2014e+00],\n",
              "        [-1.8472e+00, -2.1946e+00,  9.5208e-02, -1.3483e+00,  1.8004e+00,\n",
              "         -1.3824e-01,  7.2108e-01, -2.1147e-01, -0.0000e+00,  1.2980e+00,\n",
              "          1.1335e+00, -0.0000e+00, -1.0884e+00,  1.8093e-01,  2.3786e+00,\n",
              "          4.8799e-01],\n",
              "        [-4.9216e-02, -1.0018e+00, -6.1943e-01, -0.0000e+00,  0.0000e+00,\n",
              "         -1.4450e+00, -1.7921e-01, -0.0000e+00, -1.0695e-01,  1.7431e+00,\n",
              "          7.4613e-01, -6.0356e-01, -4.5762e-01, -1.6588e+00,  2.1191e+00,\n",
              "          2.5819e+00],\n",
              "        [-2.9245e-01,  8.8415e-01,  7.2506e-01,  3.3538e-01, -2.5737e+00,\n",
              "          6.6753e-01, -6.6924e-01, -1.0796e+00, -2.9368e-02,  1.4646e+00,\n",
              "          4.9836e-01, -0.0000e+00, -0.0000e+00, -3.9584e-01,  1.1754e+00,\n",
              "         -1.4337e+00],\n",
              "        [ 6.9850e-01,  9.8970e-01, -2.3084e-01, -9.1472e-01, -0.0000e+00,\n",
              "         -1.2251e+00,  7.5614e-02, -6.1119e-01, -1.0172e+00,  0.0000e+00,\n",
              "         -7.9147e-01,  1.6184e+00,  1.8285e+00, -0.0000e+00,  0.0000e+00,\n",
              "         -5.4022e-01],\n",
              "        [-7.7902e-01, -0.0000e+00, -6.1084e-01,  9.8369e-01,  1.3449e-01,\n",
              "         -1.3393e+00, -1.4581e-01, -1.2712e+00, -1.4975e+00,  0.0000e+00,\n",
              "         -1.5369e+00, -0.0000e+00,  1.9294e+00, -4.1509e-02, -5.2328e-01,\n",
              "         -0.0000e+00]])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Dropout_by_foot(input)"
      ],
      "metadata": {
        "id": "Qs9ybjRebl8I",
        "outputId": "658ea047-ee8d-44fb-d699-e9d5873872e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.4073e+00, -1.4405e+00, -3.1322e-01, -5.4235e-01,  0.0000e+00,\n",
              "          8.6501e-01, -0.0000e+00, -2.6440e+00,  4.0284e-01, -1.5792e+00,\n",
              "          4.3748e-01,  0.0000e+00,  1.4980e-01,  1.5471e+00,  1.3960e+00,\n",
              "         -3.0910e-01],\n",
              "        [-0.0000e+00, -2.1199e+00,  7.0831e-01,  9.9189e-01,  7.4855e-01,\n",
              "         -0.0000e+00, -4.2670e-01,  2.3163e+00,  0.0000e+00, -7.3187e-01,\n",
              "         -0.0000e+00,  2.2935e-01,  0.0000e+00,  1.9829e+00,  1.1829e+00,\n",
              "         -1.0546e+00],\n",
              "        [-7.6698e-01,  3.9491e-02, -0.0000e+00,  3.1052e-01,  5.4962e-01,\n",
              "          0.0000e+00,  8.0099e-01,  0.0000e+00, -1.2789e-01,  9.9055e-01,\n",
              "         -3.6208e-01,  6.5634e-02,  0.0000e+00,  2.8778e+00, -1.8361e+00,\n",
              "         -1.9834e+00],\n",
              "        [-8.4136e-01,  1.0910e+00,  1.3192e+00,  0.0000e+00, -0.0000e+00,\n",
              "         -4.8969e-01,  6.7912e-01, -0.0000e+00, -5.5777e-01,  9.3003e-01,\n",
              "          1.9012e+00,  4.2631e+00, -1.9140e+00, -1.5427e+00,  2.2747e+00,\n",
              "         -6.8941e-01],\n",
              "        [-7.1156e-01,  1.1500e+00,  0.0000e+00,  1.6123e+00, -0.0000e+00,\n",
              "          3.2090e+00, -5.9140e-01,  4.1944e-01, -2.0367e+00, -0.0000e+00,\n",
              "         -5.9979e-01, -6.2460e-01, -1.3337e+00,  0.0000e+00, -0.0000e+00,\n",
              "          0.0000e+00],\n",
              "        [-1.1669e-01,  8.5881e-01, -1.0479e+00,  1.1148e-03,  1.0524e+00,\n",
              "         -0.0000e+00,  1.2993e+00,  0.0000e+00, -3.0750e-01,  2.8781e+00,\n",
              "         -0.0000e+00, -6.2159e-02, -1.3062e+00, -1.1956e+00,  0.0000e+00,\n",
              "          8.8761e-01],\n",
              "        [ 2.0573e+00, -1.7002e+00,  4.3071e-01,  6.4983e-01, -3.2667e+00,\n",
              "         -0.0000e+00, -2.8530e-01,  3.4994e-01,  0.0000e+00,  0.0000e+00,\n",
              "          4.2251e-01,  5.6800e-01,  5.7118e-01, -1.0817e+00,  9.7663e-01,\n",
              "         -0.0000e+00],\n",
              "        [-0.0000e+00, -3.0438e+00, -9.1143e-02, -4.2483e-02,  0.0000e+00,\n",
              "          4.3646e-01, -1.1518e+00, -7.0243e-02, -7.7837e-01, -5.7965e-01,\n",
              "          0.0000e+00, -5.0318e-01,  1.5488e-01,  1.4560e+00,  0.0000e+00,\n",
              "          1.7341e+00],\n",
              "        [-1.1042e+00, -5.2364e-01, -1.0060e+00,  7.0701e-01,  7.6296e-01,\n",
              "          0.0000e+00,  2.4383e+00, -0.0000e+00, -9.6657e-02,  0.0000e+00,\n",
              "         -7.4249e-01, -1.5549e+00, -0.0000e+00, -1.2919e+00, -3.9080e-01,\n",
              "          3.0723e-01],\n",
              "        [-3.2455e-01,  1.4792e-01,  3.0495e-01,  1.4558e+00,  3.6072e-01,\n",
              "          4.8325e-01, -2.5133e-01, -0.0000e+00,  2.4025e-01, -9.6520e-01,\n",
              "         -2.3754e+00,  1.6335e-01, -8.8037e-01,  3.9340e-01,  1.9674e-01,\n",
              "          4.8170e-01],\n",
              "        [ 1.2089e+00, -0.0000e+00,  3.7701e-01, -1.3415e-01,  0.0000e+00,\n",
              "         -0.0000e+00,  9.5139e-01,  7.7288e-01,  3.9256e-01,  2.6667e-01,\n",
              "         -1.5006e-01,  4.5057e-01, -3.9254e-01, -1.3484e+00,  3.0101e-01,\n",
              "         -1.7453e+00],\n",
              "        [-0.0000e+00, -4.4794e-01, -1.9520e+00, -0.0000e+00,  1.3513e+00,\n",
              "          1.6435e-01,  1.9669e+00,  9.7679e-01, -1.3483e+00, -9.0114e-01,\n",
              "          1.8385e+00,  3.4454e-01,  8.3348e-01, -0.0000e+00, -1.4867e+00,\n",
              "         -1.4949e+00],\n",
              "        [-6.9954e-01,  6.6684e-01,  5.0861e-01,  4.9323e-01,  2.1439e-01,\n",
              "          1.0951e+00, -3.5886e-01,  1.2771e+00, -0.0000e+00, -1.3653e+00,\n",
              "          4.9003e-01,  7.4316e-01,  8.2784e-01, -1.5079e+00,  0.0000e+00,\n",
              "         -6.8395e-01],\n",
              "        [ 1.4639e+00,  1.2187e-01,  0.0000e+00,  1.0504e+00, -1.5671e+00,\n",
              "          1.2335e+00, -6.1832e-01, -1.6038e+00,  1.1940e+00,  0.0000e+00,\n",
              "         -8.3233e-01,  0.0000e+00,  0.0000e+00, -4.1718e-02, -1.3274e+00,\n",
              "         -0.0000e+00],\n",
              "        [-4.2917e-01,  1.9641e+00,  2.3952e-01,  4.7493e-01, -1.8095e-01,\n",
              "          7.9702e-01, -3.5161e-01, -1.6623e+00, -1.7751e-01, -6.6768e-01,\n",
              "         -6.5422e-01,  0.0000e+00, -0.0000e+00,  1.0485e+00,  1.4411e+00,\n",
              "         -2.2014e+00],\n",
              "        [-1.8472e+00, -2.1946e+00,  0.0000e+00, -1.3483e+00,  1.8004e+00,\n",
              "         -1.3824e-01,  7.2108e-01, -2.1147e-01, -0.0000e+00,  1.2980e+00,\n",
              "          0.0000e+00, -5.9439e-01, -0.0000e+00,  1.8093e-01,  2.3786e+00,\n",
              "          4.8799e-01],\n",
              "        [-4.9216e-02, -1.0018e+00, -6.1943e-01, -0.0000e+00,  0.0000e+00,\n",
              "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -1.0695e-01,  1.7431e+00,\n",
              "          7.4613e-01, -6.0356e-01, -4.5762e-01, -1.6588e+00,  2.1191e+00,\n",
              "          2.5819e+00],\n",
              "        [-2.9245e-01,  0.0000e+00,  7.2506e-01,  3.3538e-01, -2.5737e+00,\n",
              "          6.6753e-01, -6.6924e-01, -1.0796e+00, -2.9368e-02,  1.4646e+00,\n",
              "          0.0000e+00, -0.0000e+00, -1.4449e+00, -3.9584e-01,  1.1754e+00,\n",
              "         -0.0000e+00],\n",
              "        [ 6.9850e-01,  0.0000e+00, -2.3084e-01, -9.1472e-01, -0.0000e+00,\n",
              "         -0.0000e+00,  0.0000e+00, -0.0000e+00, -1.0172e+00,  0.0000e+00,\n",
              "         -7.9147e-01,  1.6184e+00,  1.8285e+00, -7.7554e-01,  1.2355e+00,\n",
              "         -5.4022e-01],\n",
              "        [-7.7902e-01, -2.7031e-01, -6.1084e-01,  9.8369e-01,  1.3449e-01,\n",
              "         -1.3393e+00, -1.4581e-01, -1.2712e+00, -1.4975e+00,  5.9805e-01,\n",
              "         -1.5369e+00, -1.7125e+00,  1.9294e+00, -4.1509e-02, -5.2328e-01,\n",
              "         -3.1950e-01]])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment mit Weights&Biases\n",
        "\n",
        "Optional: Spielen Sie das Beispiel durch und notieren Sie Fragen und Erkenntnisse. Sie benötigen um das Beispiel sinnvoll nutzen zu können einen Weights&Biases Account - wenn Sie sich diesen nicht anlegen möchten ist das kein Problem, wir werden das Beispiel auch nochmal gemeinsam kurz durchgehen.\n",
        "\n",
        "\n",
        "https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/How_does_adding_dropout_affect_model_performance.ipynb\n"
      ],
      "metadata": {
        "id": "Y_A2DzKXWg44"
      }
    }
  ]
}